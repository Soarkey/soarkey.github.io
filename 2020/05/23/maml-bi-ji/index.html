<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="MAML笔记"><meta name="keywords" content="Meta-Learning,Object Detection"><meta name="author" content="Soarkey"><meta name="copyright" content="Soarkey"><title>MAML笔记 | Soarkey's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.1"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#mamltracking-by-instance-detection-a-meta-learning-approach"><span class="toc-text"> 【MAML】Tracking by Instance Detection: A Meta-Learning Approach</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-introduction"><span class="toc-text"> 1. Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-related-work"><span class="toc-text"> 2. Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-cnn-based-visual-object-tracking"><span class="toc-text"> 2.1 CNN-based visual object tracking</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-meta-learning-and-its-application-to-tracking"><span class="toc-text"> 2.2 Meta learning and its application to tracking</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-learning-an-instance-detector-with-maml"><span class="toc-text"> 3. Learning an Instance Detector with MAML</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-retina-maml-and-fcos-maml"><span class="toc-text"> 4. Retina-MAML and FCOS-MAML</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#41-detectors"><span class="toc-text"> 4.1 Detectors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#42-offline-maml-training"><span class="toc-text"> 4.2 Offline MAML training</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#参考资料"><span class="toc-text"> 参考资料</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars3.githubusercontent.com/u/17349395?s=400&amp;u=b4d8f5e790e0fab115d7237ee3404f24813e115b&amp;v=4"></div><div class="author-info__name text-center">Soarkey</div><div class="author-info__description text-center">Soarkey's Blog</div><div class="follow-button"><a href="https://github.com/Soarkey">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">7</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">9</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">8</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://github.com/Soarkey">github</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/12/20/kXR6KLoOcwDsUQy.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Soarkey's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">MAML笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-23</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Meta-Learning/">Meta Learning</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Meta-Learning/Object-Detection/">Object Detection</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="mamltracking-by-instance-detection-a-meta-learning-approach"><a class="markdownIt-Anchor" href="#mamltracking-by-instance-detection-a-meta-learning-approach"></a> 【MAML】Tracking by Instance Detection: A Meta-Learning Approach</h1>
<blockquote>
<p><img src="/2020/05/23/maml-bi-ji/image-20200501141721053.png" alt="image-20200501141721053"></p>
<p>paper: <a href="https://arxiv.org/abs/2004.00830">https://arxiv.org/abs/2004.00830</a></p>
</blockquote>
<p>MAML :  model-agnostic meta-learning</p>
<a id="more"></a>
<h1 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1. Introduction</h1>
<p>​	跟踪可以看成是一个特殊的检测问题，即instance detection。主要的区别在于目标检测可以定位某些预定义类别的对象，并且其输出不会在intra-class实例之间进行区分；但是目标追踪仅查找特定实例，该实例可能属于初始帧中指定的任何已知或未知的对象类别。</p>
<p>​	因为两者的相似性，很多目标检测的技术可以应用在目标追踪上面，例如，Faster R-CNN中的RPN网络应用在SiamRPN追踪器及其变体；multi-aspect-ratio anchors解决了困扰之前追踪器的box estimation问题；IoU-Net也应用于ATOM和DiMP中。</p>
<p>​	<strong>给定合适的初始化，检测器从单帧中快速学习实例信息从而转变成一个跟踪器</strong>。难点就在于如何给这个初始化使得模型在少量样本的情况下就能学到一个新的instance的信息而不会过拟合，这就是一个domain adaptation的问题，而model-agnostic meta-learning (MAML) 无疑是解决这种问题的一个利器。</p>
<p>​	这样，我们就可以将一个检测器通过domain adaptation转变成适合某个特定domain （或者说是instance）的跟踪器，下图形象地展示了这个过程。</p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200503174910896.png" alt="image-20200503174910896"></p>
<p>具体应用MAML的将detector转变为tracker的流程如下：</p>
<ol>
<li>
<p>使用gradient descent训练一般的检测器；</p>
</li>
<li>
<p>使用MAML在大量跟踪序列上离线训练；</p>
</li>
<li>
<p>给定测试序列第一帧信息后，通过几步gradient descent 来 fine-tune模型参数做domain adaptation从而得到一个跟踪器。跟踪过程中，当目标外观发生变化时，可以用更多样本进行fine-tune得到更好的适应能力。</p>
</li>
</ol>
<p>​	作者设计了两个instance detector分别为Retina-MAML和 FCOS-MAML代表着anchor-base和anchor-free，分别来源于RetinaNet和FCOS，均取得了不错的结果，并且速度达到实时的40FPS。</p>
<blockquote>
<p><strong>RetinaNet(2018):</strong></p>
<p>paper: <a href="https://arxiv.org/pdf/1708.02002.pdf">https://arxiv.org/pdf/1708.02002.pdf</a></p>
<p>blog:</p>
<ul>
<li>
<p>目标检测算法 - RetinaNet <a href="https://zhuanlan.zhihu.com/p/67768433">https://zhuanlan.zhihu.com/p/67768433</a></p>
</li>
<li>
<p>RetinaNet: Focal loss在目标检测网络中的应用 <a href="https://www.jianshu.com/p/8e501a159b28">https://www.jianshu.com/p/8e501a159b28</a></p>
</li>
<li>
<p>Review: RetinaNet — Focal Loss (Object Detection) <a href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4">https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4</a></p>
</li>
</ul>
<p><img src="/2020/05/23/maml-bi-ji/5971313-152cb2d4c07fce20.JPG" alt="img"></p>
<p><strong>FCOS:</strong></p>
<p>paper: <a href="https://arxiv.org/pdf/1904.01355.pdf">https://arxiv.org/pdf/1904.01355.pdf</a></p>
<p>blog:</p>
<ul>
<li>FCOS:一阶全卷积目标检测 <a href="https://zhuanlan.zhihu.com/p/63868458">https://zhuanlan.zhihu.com/p/63868458</a></li>
<li>FCOS: 最新的one-stage逐像素目标检测算法 <a href="https://blog.csdn.net/qiu931110/article/details/89073244">https://blog.csdn.net/qiu931110/article/details/89073244</a></li>
</ul>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508095947264.png" alt="image-20200508095947264"></p>
</blockquote>
<h1 id="2-related-work"><a class="markdownIt-Anchor" href="#2-related-work"></a> 2. Related Work</h1>
<h2 id="21-cnn-based-visual-object-tracking"><a class="markdownIt-Anchor" href="#21-cnn-based-visual-object-tracking"></a> 2.1 CNN-based visual object tracking</h2>
<p>​	首先介绍CNN-based trackers的一些工作，这里作者按照是否显式的使用模板将其分成了两类。</p>
<p>​	template-based的方法通常有一个模板分支，将目标外观信息存储在显式模板中，如常见的siamese系列的跟踪器。这种方法优点是快，不需要在线更新。难点也是难以找到一个合适的在线更新方法来更新模板，从而无法适应一些目标的变化。(SiamFC, SiamRPN, SPM-Tracker, ATOM/DiMP)</p>
<p>​	另一方面template-free的方法就是将目标外观信息以微调参数的形式存储在网络中，比如MDNet。但这类算法的难点一个是慢，一个是容易过拟合，无法很好将instance信息融合进网络。</p>
<p>​	本文设计了一个template-free trackers，作者认为这类方法的整体架构可以很好的匹配一个检测框架，不需要额外添加其他分支改变结构。</p>
<h2 id="22-meta-learning-and-its-application-to-tracking"><a class="markdownIt-Anchor" href="#22-meta-learning-and-its-application-to-tracking"></a> 2.2 Meta learning and its application to tracking</h2>
<p>​	介绍一些meta-learning的相关工作以及在跟踪中的应用，不过目前这种方法在跟踪中的应用还不够成熟，有些精度不够高，有些速度太慢了。</p>
<p>​	Model-agnostic meta-learning (MAML) [10] is an im-<br>
portant algorithm for meta learning.  The most striking merit of MAML is that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems.<br>
Because of this, MAML is a perfect candidate to realize our idea, which is to convert any advanced object detectors (trained with gradient descent) into a tracker.</p>
<p>MAML++</p>
<p>MetaSGD</p>
<p>​	Meta-Tracker第一次使用MAML作为MDNet的domain adaptation step；MetaRTT进一步应用MAML作为在线更新步骤，以此加速现有tracker的在线训练速度。</p>
<p>​	We argue that, since meta learning provides a mechanism to quickly adapt a deep network to model a particular object and avoid overfitting, why not directly convert a modern object detector into a tracker, instead of making a slow tracker faster?</p>
<blockquote>
<p>ICCV 2019 Lianghua Huang, Xin Zhao, and Kaiqi Huang. <strong>Bridging the gap between detection and tracking: A unified approach</strong></p>
<p>paper: <a href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Bridging_the_Gap_Between_Detection_and_Tracking_A_Unified_Approach_ICCV_2019_paper.pdf</a></p>
<p>blog:</p>
<ul>
<li>[Note2] 目标检测框架在目标跟踪中的应用 <a href="https://www.bilibili.com/read/cv4707398">https://www.bilibili.com/read/cv4707398</a></li>
</ul>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508101652361.png" alt="image-20200508101652361"></p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508101910940.png" alt="image-20200508101910940"></p>
</blockquote>
<h1 id="3-learning-an-instance-detector-with-maml"><a class="markdownIt-Anchor" href="#3-learning-an-instance-detector-with-maml"></a> 3. Learning an Instance Detector with MAML</h1>
<p>要解决的问题：如何通过MAML来学习一个instance detector？</p>
<p>​	将一个detector转化为instance detector的关键在于提供一个好的初始化。</p>
<p><mark>[➡pdf]</mark></p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508173805473.png" alt="image-20200508173805473"></p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508205138579.png" alt="image-20200508205138579"></p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508173826254.png" alt="image-20200508173826254"></p>
<p>​	整体的流程如下图所示，这里训练的时候简化了，仅选择一对图片来构建一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">D_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，第一张图通过缩放得到3张图作为support set，第二张图就直接作为target set。</p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200502092252377.png" alt="image-20200502092252377"></p>
<p>​	为了稳定训练，作者添加了一些meta-learning中用到的操作。</p>
<p><mark><strong>Multi-step loss optimization</strong></mark></p>
<p>​	参照MAML++，将inner-level optimization中GD的每一个step的参数都用来优化target set中的loss，而不是只用最后一步得到的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">θ_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。则公式2可以改写为：</p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508173437682.png" alt="image-20200508173437682"></p>
<p>​	值得注意的是初始参数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">θ_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>同样被用于计算outer-level loss。</p>
<p><mark><strong>Kernel-wise learnable learning rate</strong></mark></p>
<p>​	参照MetaSGD，将公式1中inner-level optimization的学习率<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>变为可学习的：</p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200508174139349.png" alt="image-20200508174139349"></p>
<p>​	<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>θ</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\theta_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>有相同的size，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord">⊙</span></span></span></span>表示element-wise product，若为每个参数设置学习率会使模型大小增加一倍，所以作者这里使用了kernel-wise的方法，给每个卷积核定义了学习率。（大概）</p>
<h1 id="4-retina-maml-and-fcos-maml"><a class="markdownIt-Anchor" href="#4-retina-maml-and-fcos-maml"></a> 4. Retina-MAML and FCOS-MAML</h1>
<p>​	这一节就是具体将如何通过开始提到的3个步骤来构建跟踪器。包括detector choices, offline training details 和 online tracking process。</p>
<h2 id="41-detectors"><a class="markdownIt-Anchor" href="#41-detectors"></a> 4.1 Detectors</h2>
<p>Anchor-based vs Anchor-free</p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200502092323876.png" alt="image-20200502092323876"></p>
<p>​	作者分别选取了anchor-base的RetinaNet和anchor-free的FCOS来作为检测器。由于跟踪通常会根据目标大小选择一个搜索区域然后将其resize到固定大小输入到网络中，<mark>所以目标尺度在跟踪任务中通常分布比较集中，不需要FPN这样的结构</mark>。另外对于FCOS只保留了centerness 来预测分类得分（FCOS用了三个network head：一个common regression head和两个centerness/classification heads）；Retinanet只设计了一个64×64大小的anchor。</p>
<p>​	整体网络结构如下图4，Resnet18作为backbone，前3个block在ImageNet预训练后在离线训练时冻结，block5被丢弃，block4（黄色部分）复制一份分别独立作用在分类和回归上。</p>
<p><img src="/2020/05/23/maml-bi-ji/image-20200502092349927.png" alt="image-20200502092349927"></p>
<h2 id="42-offline-maml-training"><a class="markdownIt-Anchor" href="#42-offline-maml-training"></a> 4.2 Offline MAML training</h2>
<p><strong>Loss definition:</strong></p>
<ul>
<li>
<p>Retina-MAML</p>
<p>Focal loss (classification) + smooth L1 loss (regression)</p>
</li>
<li>
<p>FCOS-MAML</p>
<p>L2 loss (centerness scores) + L1 loss (regression)</p>
</li>
</ul>
<p><strong>Training data:</strong></p>
<p>​	MS-COCO, GOT10k,  TrackingNet, LaSOT-train</p>
<ul>
<li>
<p>cropped and resized into <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>263</mn><mo>×</mo><mn>263</mn></mrow><annotation encoding="application/x-tex">263\times263</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mord">3</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord">6</span><span class="mord">3</span></span></span></span></p>
</li>
<li>
<p>data augmentation: random scaling, shifting</p>
</li>
</ul>
<p><strong>Optimization:</strong></p>
<h1 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h1>
<ul>
<li>What is Model-Agnostic Meta-learning (MAML) ?  <a href="https://towardsdatascience.com/model-agnostic-meta-learning-maml-8a245d9bc4ac">https://towardsdatascience.com/model-agnostic-meta-learning-maml-8a245d9bc4ac</a></li>
<li>【paper】Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks <a href="https://arxiv.org/pdf/1703.03400.pdf">https://arxiv.org/pdf/1703.03400.pdf</a></li>
<li>【论文翻译】Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks <a href="https://blog.csdn.net/weixin_40523230/article/details/85005378">https://blog.csdn.net/weixin_40523230/article/details/85005378</a></li>
<li>【video】MAML: Model-Agnostic Meta-Learning <a href="https://youtu.be/IkDw22a8BDE">https://youtu.be/IkDw22a8BDE</a></li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Soarkey</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://soarkey.github.io/2020/05/23/maml-bi-ji/">http://soarkey.github.io/2020/05/23/maml-bi-ji/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Meta-Learning/">Meta-Learning</a><a class="post-meta__tags" href="/tags/Object-Detection/">Object Detection</a></div><div class="social-share pull-right"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2020/05/26/ltmu-bi-ji/"><i class="fa fa-chevron-left">  </i><span>LTMU笔记</span></a></div><div class="next-post pull-right"><a href="/2020/05/18/dimp/"><span>DiMP</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer class="footer-bg" style="background-image: url(https://i.loli.net/2019/12/20/kXR6KLoOcwDsUQy.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2020 By Soarkey</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script src="/js/search/local-search.js"></script><script id="ribbon" src="/js/third-party/canvas-ribbon.js" size="150" alpha="0.2" zIndex="-1" data-click="true"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>